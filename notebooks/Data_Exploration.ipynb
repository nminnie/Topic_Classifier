{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWQ8WUZVevvJ",
        "colab_type": "text"
      },
      "source": [
        "## Mount to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okPWBhW0e-Ev",
        "colab_type": "code",
        "outputId": "5067da77-ad21-4c85-8ac2-fc4d2d30429c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auLf3EEpfAkQ",
        "colab_type": "code",
        "outputId": "97fe12f8-a50c-48fd-a172-c680f76b011c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6sBBQzNwxgP",
        "colab_type": "text"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_otEM74BeuIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h75cqGLBw0Pd",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPUUAHTxRMmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data_cleaning import clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYmydhd6w8Pq",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9eseHSCViFN",
        "colab_type": "code",
        "outputId": "c1238d33-26a3-404a-a025-43a7abd87c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/Colab\\ Notebooks/data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZflsYhHfGvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file = \"reviews_8_categories.txt\"\n",
        "encoding=\"utf-8\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3cDdUWEfRka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = [], []\n",
        "with open(data_file, \"r\") as infile:\n",
        "    for line in infile:\n",
        "        label, text = line.split(\"\\t\")\n",
        "        text = clean_text(text)\n",
        "\n",
        "        X.append(text)\n",
        "        y.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wex_wyTfeNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({\"Review\": X, \"Label\":y})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iicwu4lw_g9",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9g0Rzr_fl0B",
        "colab_type": "code",
        "outputId": "c1f0508f-0fee-4554-bbf1-95d73001e48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df['Sentiment'] = df['Review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Label</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>these rings are difficult to install the scope...</td>\n",
              "      <td>sports</td>\n",
              "      <td>0.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it is great to see ashanti back doing what she...</td>\n",
              "      <td>music</td>\n",
              "      <td>0.354537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this series has been packed full of explosive ...</td>\n",
              "      <td>books</td>\n",
              "      <td>0.152500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>just make sure to buy lots of mantels you do n...</td>\n",
              "      <td>sports</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>these wallets are certainly well made but the ...</td>\n",
              "      <td>sports</td>\n",
              "      <td>0.108958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review   Label  Sentiment\n",
              "0  these rings are difficult to install the scope...  sports   0.052000\n",
              "1  it is great to see ashanti back doing what she...   music   0.354537\n",
              "2  this series has been packed full of explosive ...   books   0.152500\n",
              "3  just make sure to buy lots of mantels you do n...  sports   0.500000\n",
              "4  these wallets are certainly well made but the ...  sports   0.108958"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR4f3wGogBr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment_examples(sentiment, n=5):\n",
        "  return df.loc[df['Sentiment'] == sentiment, ['Review']].sample(n).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx-xSbYxCMW",
        "colab_type": "text"
      },
      "source": [
        "### Most negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDn9SMjEhhnD",
        "colab_type": "code",
        "outputId": "e9701c2c-5bec-4c41-e575-c2121e1bd1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "get_sentiment_examples(-1, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['worst purchase ever save your money and buy a zenmuse hd3d'],\n",
              "       [\"never quite lived up to my expectations but it was set in prague soy'knowbonus points for that i still do not get why the cover had to be so terrible\"],\n",
              "       ['not the greatestnot the worst'],\n",
              "       ['taste is horrible reminds me of liquid cough medicine from when i was a kid'],\n",
              "       ['terrible quality the pictures do not transfer on to the skin so the kids ended up with bits and pieces of their tattoos'],\n",
              "       ['terrible face does not match the body neither does it matches gal gadot'],\n",
              "       ['just nasty'],\n",
              "       ['awful'],\n",
              "       ['nasty tasting'],\n",
              "       ['awful']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrVw5GGBxEjw",
        "colab_type": "text"
      },
      "source": [
        "### Neutral reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt3Pc3pjhji7",
        "colab_type": "code",
        "outputId": "faa34368-7a03-4bea-ef93-d325ee149bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "get_sentiment_examples(0, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['everything made for shotguns is made for 12ga i cut a bicycle tube up and modified this to accomodate 16ga works like it supposed to extra ammo where and when you need it'],\n",
              "       ['xlint'],\n",
              "       ['they work well - just note that there is a burr on one side from production'],\n",
              "       ['me encanta este set de boxer ajustado a la medida y confort total lo recomiendo jesus loreto valencia venezuela xd'],\n",
              "       ['awsome'],\n",
              "       ['ellie goulding is my preferred artist above madonna beyonce etc'],\n",
              "       ['error never ordered'],\n",
              "       ['i may have given in and ordered the next two booksopps'],\n",
              "       ['i bought this for my niece my sister and her husband blew it up and it survived a toddler for one day then busted'],\n",
              "       ['my husband died eight years ago and i always considered this our song i will always think of him when i hear it']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZ8zXqFxHY_",
        "colab_type": "text"
      },
      "source": [
        "### Most positive reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_I_8nJ6hmTK",
        "colab_type": "code",
        "outputId": "59ee48e6-fe2d-4553-d432-46b48a5973bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "get_sentiment_examples(1, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['these are the best i have ever ate'],\n",
              "       ['in one word excellent the best quality of jigsawpuzzles one of the best buys in amazon'],\n",
              "       ['one of mjs best and creepiest sounding songs but hey mj was a weirdo and his lyrics shows but hey the world didn not care'],\n",
              "       ['perfect toy'],\n",
              "       ['excellent song'],\n",
              "       ['this song is awesome'],\n",
              "       ['awesome song'],\n",
              "       ['excellent'],\n",
              "       ['best salads ever i tell everyone to buy them'],\n",
              "       ['awesome']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDEY_CUnxLdb",
        "colab_type": "text"
      },
      "source": [
        "## Word count\n",
        "\n",
        "Most utterances are under 500 words in length. There are exceptions with about 3500 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyZ1X-b4L_o",
        "colab_type": "code",
        "outputId": "16f2ed7f-6228-49e4-d797-9c9ec0384b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "df['Word count'] = df['Review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.hist(df['Word count'], bins=10)\n",
        "plt.title(\"Number of words per utterance\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Number of words')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Number of words')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHDc_vqOxaOr",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment and Word Count per Category \n",
        "On average, **book reviews are the longest and most negative**, while **music reviews are the shortest and most positive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PczbUoirh6iN",
        "colab_type": "code",
        "outputId": "422b8240-3046-4d31-fe99-4cfae9e2cdaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "df.groupby('Label').mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Word count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>books</th>\n",
              "      <td>0.176161</td>\n",
              "      <td>198.2522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clothing</th>\n",
              "      <td>0.253568</td>\n",
              "      <td>60.6247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>0.351199</td>\n",
              "      <td>38.7687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>instruments</th>\n",
              "      <td>0.332483</td>\n",
              "      <td>57.2598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movies</th>\n",
              "      <td>0.217940</td>\n",
              "      <td>165.5668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>music</th>\n",
              "      <td>0.419204</td>\n",
              "      <td>31.4948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>0.226197</td>\n",
              "      <td>89.1470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toys</th>\n",
              "      <td>0.307194</td>\n",
              "      <td>42.1148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Sentiment  Word count\n",
              "Label                             \n",
              "books         0.176161    198.2522\n",
              "clothing      0.253568     60.6247\n",
              "food          0.351199     38.7687\n",
              "instruments   0.332483     57.2598\n",
              "movies        0.217940    165.5668\n",
              "music         0.419204     31.4948\n",
              "sports        0.226197     89.1470\n",
              "toys          0.307194     42.1148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVXuETXvxxJP",
        "colab_type": "text"
      },
      "source": [
        "## BoW and TFIDF n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnSD_d5-5P_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_ngrams(corpus, criteria, ngram_range, stop_words='english', n=10):\n",
        "    corpus = corpus['Review']\n",
        "    assert criteria == 'bow' or criteria == 'tfidf', \"Invalid criteria\"\n",
        "    if criteria == 'bow':\n",
        "      vec = CountVectorizer(ngram_range=ngram_range, stop_words=stop_words).fit(corpus)\n",
        "    elif criteria == 'tfidf':\n",
        "      vec = TfidfVectorizer(ngram_range=ngram_range, stop_words=stop_words).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, round(sum_words[0, idx], 2)) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTmV3s7Vx4qv",
        "colab_type": "text"
      },
      "source": [
        "### Most common **BoW** n-grams in all the reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-bcwkrri9Z3",
        "colab_type": "code",
        "outputId": "101e8138-422d-41a0-f2d3-034f075247d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "top_BoW_unigrams_all = get_top_ngrams(df, 'bow', (1,1))\n",
        "top_BoW_bigrams_all = get_top_ngrams(df, 'bow', (2,2))\n",
        "print(f\"Top BoW unigrams - All reviews:\\n{top_BoW_unigrams_all}\\n\\nTop BoW bigrams - All reviews:\\n{top_BoW_bigrams_all}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top BoW unigrams - All reviews:\n",
            "[('like', 28789), ('just', 24695), ('great', 24215), ('good', 22628), ('book', 20344), ('love', 18602), ('really', 18426), ('time', 13496), ('movie', 12055), ('does', 11779)]\n",
            "\n",
            "Top BoW bigrams - All reviews:\n",
            "[('year old', 2126), ('highly recommend', 1177), ('really like', 1061), ('feel like', 1060), ('read book', 967), ('hide spoiler', 920), ('good quality', 901), ('felt like', 894), ('long time', 875), ('works great', 842)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPyvBYJyC3s",
        "colab_type": "text"
      },
      "source": [
        "### Most common **TFIDF** n-grams in all the reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvtovurptGq6",
        "colab_type": "code",
        "outputId": "5e55a13f-05d7-4248-a3c8-95236787d9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "top_tfidf_unigrams_all = get_top_ngrams(df, 'tfidf', (1,1))\n",
        "top_tfidf_bigrams_all = get_top_ngrams(df, 'tfidf', (2,2))\n",
        "print(f\"Top TFIDF unigrams - All reviews:\\n{top_tfidf_unigrams_all}\\n\\nTop TFIDF bigrams - All reviews:\\n{top_tfidf_bigrams_all}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top TFIDF unigrams - All reviews:\n",
            "[('great', 2918.32), ('good', 2348.09), ('love', 2108.69), ('song', 1646.93), ('like', 1616.45), ('just', 1352.13), ('really', 1133.75), ('book', 1063.47), ('nice', 1003.56), ('product', 890.74)]\n",
            "\n",
            "Top TFIDF bigrams - All reviews:\n",
            "[('great song', 441.42), ('love song', 266.7), ('great product', 199.77), ('year old', 196.11), ('works great', 178.17), ('good song', 170.1), ('good quality', 163.87), ('great price', 146.36), ('good product', 122.11), ('highly recommend', 117.47)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilcWbKHTyHEl",
        "colab_type": "text"
      },
      "source": [
        "## Top N-grams per Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxPTYbGFj_NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_reviews_by_label(label, dataframe=df):\n",
        "  return dataframe.loc[dataframe['Label'] == label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISIYyJ7sq07p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_ngrams_by_label(label):\n",
        "    reviews_df = get_reviews_by_label(label)\n",
        "\n",
        "    unigrams = get_top_ngrams(reviews_df, 'tfidf', (1,1))\n",
        "    bigrams = get_top_ngrams(reviews_df, 'tfidf', (2,2))\n",
        "\n",
        "    print(f\"Unigrams: {unigrams}\\n\\nBigrams: {bigrams}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJoJUDleWUOu",
        "colab_type": "text"
      },
      "source": [
        "### Book reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC9BItT1NcS-",
        "colab_type": "code",
        "outputId": "71683931-6a87-4071-c589-ccec4a7a26bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('books')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('book', 527.0), ('read', 322.41), ('really', 273.04), ('just', 271.96), ('story', 270.14), ('like', 268.43), ('love', 237.0), ('series', 213.46), ('characters', 196.95), ('good', 193.95)]\n",
            "\n",
            "Bigrams: [('review come', 53.32), ('read book', 38.24), ('hide spoiler', 36.02), ('loved book', 30.17), ('felt like', 30.07), ('really enjoyed', 27.57), ('main character', 26.59), ('feel like', 25.9), ('really liked', 25.85), ('35 stars', 25.15)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGCFqr0yNOl",
        "colab_type": "text"
      },
      "source": [
        "### Movie reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59uQIDw-t8DF",
        "colab_type": "code",
        "outputId": "d388c7d2-0b8f-4124-be36-38c38c825b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('movies')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('movie', 456.76), ('film', 264.49), ('good', 246.5), ('great', 243.93), ('like', 200.51), ('just', 178.59), ('story', 175.85), ('love', 170.71), ('watch', 166.61), ('dvd', 158.04)]\n",
            "\n",
            "Bigrams: [('good movie', 32.63), ('great movie', 32.58), ('blu ray', 26.48), ('story line', 21.64), ('special effects', 21.01), ('highly recommend', 19.83), ('love movie', 19.5), ('movie good', 17.65), ('sci fi', 16.84), ('movie great', 16.52)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1B9OlAiyZdN",
        "colab_type": "text"
      },
      "source": [
        "### Food & Groceries reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZqYSU-NuJaj",
        "colab_type": "code",
        "outputId": "d8f28cd2-0491-4922-a48b-b77c6df32c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('food')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('good', 504.34), ('great', 490.1), ('love', 328.92), ('taste', 261.69), ('like', 248.02), ('product', 231.96), ('flavor', 219.53), ('coffee', 211.54), ('delicious', 184.66), ('tea', 174.85)]\n",
            "\n",
            "Bigrams: [('great product', 64.96), ('great taste', 41.0), ('good product', 40.77), ('great price', 34.03), ('good stuff', 32.09), ('taste great', 31.72), ('good price', 28.41), ('taste good', 28.29), ('tastes great', 27.48), ('good flavor', 26.58)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh6gEkxoySuu",
        "colab_type": "text"
      },
      "source": [
        "### Music reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEdDHTPtnfrK",
        "colab_type": "code",
        "outputId": "558bb1d8-fc59-40d9-ea32-9f36c1115eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('music')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('song', 937.82), ('great', 895.62), ('love', 759.11), ('good', 529.02), ('music', 340.24), ('like', 283.05), ('songs', 190.59), ('awesome', 167.81), ('album', 154.69), ('just', 153.79)]\n",
            "\n",
            "Bigrams: [('great song', 417.94), ('love song', 244.54), ('good song', 164.15), ('great music', 65.06), ('good music', 61.35), ('like song', 52.05), ('awesome song', 49.44), ('song great', 34.46), ('love music', 32.94), ('song love', 32.38)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGickeuayUjT",
        "colab_type": "text"
      },
      "source": [
        "### Musical instrument reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMXvymzauF3E",
        "colab_type": "code",
        "outputId": "85f85744-bcb1-4bee-e135-beee7b4affb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('instruments')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('great', 526.3), ('good', 402.7), ('works', 282.87), ('strings', 257.75), ('guitar', 221.37), ('nice', 217.93), ('price', 213.46), ('sound', 208.92), ('product', 198.98), ('quality', 185.66)]\n",
            "\n",
            "Bigrams: [('works great', 106.66), ('great product', 57.49), ('good quality', 48.61), ('great price', 47.3), ('good product', 42.47), ('great strings', 36.78), ('good price', 35.22), ('does job', 32.04), ('easy use', 26.22), ('work great', 25.46)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHUYCURWb2u",
        "colab_type": "text"
      },
      "source": [
        "### Clothing reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dfa0ce6b-fcfa-42db-d340-dc32eb947d65",
        "id": "RwlX98W_NSXn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('clothing')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('great', 288.05), ('fit', 275.48), ('size', 272.68), ('like', 261.3), ('wear', 254.55), ('love', 252.81), ('good', 232.79), ('comfortable', 228.76), ('just', 213.97), ('shoes', 198.15)]\n",
            "\n",
            "Bigrams: [('true size', 37.62), ('good quality', 31.01), ('really like', 25.86), ('fit perfectly', 23.35), ('looks great', 22.34), ('fit great', 21.7), ('highly recommend', 21.58), ('just right', 21.21), ('look great', 19.72), ('look like', 18.9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89MsH4EMWg4m",
        "colab_type": "text"
      },
      "source": [
        "### Sports & Gym reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9p1i5pcNbxq",
        "colab_type": "code",
        "outputId": "e04a1e06-e8b0-4107-ae23-5ab0bbe37037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('sports')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('great', 253.84), ('good', 223.88), ('use', 199.95), ('like', 192.34), ('just', 182.01), ('knife', 159.78), ('easy', 143.97), ('price', 142.83), ('works', 141.24), ('does', 138.53)]\n",
            "\n",
            "Bigrams: [('works great', 36.14), ('highly recommend', 24.81), ('good quality', 24.62), ('easy install', 20.91), ('easy use', 18.36), ('high quality', 17.07), ('great price', 16.9), ('great product', 16.86), ('just fine', 16.42), ('light weight', 15.25)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5vVe3FfyW0T",
        "colab_type": "text"
      },
      "source": [
        "### Toys & Games reviews: most common n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQQWB16uHjg",
        "colab_type": "code",
        "outputId": "a3dfe4d3-b35d-4ff4-9e4d-358c2624109c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print_ngrams_by_label('toys')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams: [('great', 427.75), ('love', 307.34), ('loves', 283.9), ('good', 267.03), ('fun', 219.72), ('old', 218.51), ('loved', 218.31), ('cute', 208.84), ('kids', 197.69), ('year', 188.26)]\n",
            "\n",
            "Bigrams: [('year old', 117.7), ('kids love', 59.75), ('daughter loves', 55.45), ('grandson loves', 50.7), ('son loves', 50.08), ('great product', 45.51), ('grandson loved', 39.81), ('good quality', 38.34), ('old loves', 37.12), ('kids loved', 32.73)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}